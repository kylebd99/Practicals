{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "try:\n",
    "\timport xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "\timport xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "import util\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "\t\"\"\"\n",
    "\targuments:\n",
    "\t  ffs are a list of feature-functions.\n",
    "\t  direc is a directory containing xml files (expected to be train or test).\n",
    "\t  global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "\t  should only be provided when extracting features from test data, so that \n",
    "\t  the columns of the test matrix align correctly.\n",
    "\n",
    "\treturns: \n",
    "\t  a sparse design matrix, a dict mapping features to column-numbers,\n",
    "\t  a vector of target classes, and a list of system-call-history ids in order \n",
    "\t  of their rows in the design matrix.\n",
    "\t  \n",
    "\t  Note: the vector of target classes returned will contain the true indices of the\n",
    "\t  target classes on the training data, but will contain only -1's on the test\n",
    "\t  data\n",
    "\t\"\"\"\n",
    "\tfds = [] # list of feature dicts\n",
    "\tclasses = []\n",
    "\tids = [] \n",
    "\tfor datafile in os.listdir(direc):\n",
    "\t\t# extract id and true class (if available) from filename\n",
    "\t\tid_str,clazz = datafile.split('.')[:2]\n",
    "\t\tids.append(id_str)\n",
    "\t\t# add target class if this is training data\n",
    "\t\ttry:\n",
    "\t\t\tclasses.append(util.malware_classes.index(clazz))\n",
    "\t\texcept ValueError:\n",
    "\t\t\t# we should only fail to find the label in our list of malware classes\n",
    "\t\t\t# if this is test data, which always has an \"X\" label\n",
    "\t\t\tassert clazz == \"X\"\n",
    "\t\t\tclasses.append(-1)\n",
    "\t\trowfd = {}\n",
    "\t\t# parse file as an xml document\n",
    "\t\ttree = ET.parse(os.path.join(direc,datafile))\n",
    "\t\t# accumulate features\n",
    "\t\t[rowfd.update(ff(tree)) for ff in ffs]\n",
    "\t\tfds.append(rowfd)\n",
    "\t\t\n",
    "\tX,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "\treturn X, feat_dict, np.array(classes), ids\n",
    "\n",
    "def extractText(ff,direc=\"train\", testDict = None):\n",
    "    ids = []\n",
    "    classes = []\n",
    "    classesDict = {}\n",
    "    fds = []\n",
    "    secondTime = False\n",
    "    if testDict == None:\n",
    "        globalDict = dict()\n",
    "        globalDict[\"counter\"] = 0\n",
    "    else:\n",
    "        globalDict = testDict\n",
    "        secondTime = True\n",
    "    for datafile in os.listdir(direc):\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        if clazz in classesDict:\n",
    "            if classesDict[clazz] < 62:\n",
    "                classesDict[clazz] += 1\n",
    "                ids.append(id_str)\n",
    "                try:\n",
    "                    classes.append(util.malware_classes.index(clazz))\n",
    "                except ValueError:\n",
    "                    assert clazz == \"X\"\n",
    "                    classes.append(-1)\n",
    "                tree = ET.parse(os.path.join(direc,datafile))\n",
    "                fds.append(ff(tree, globalDict,secondTime))\n",
    "                del tree\n",
    "        else:\n",
    "            classesDict[clazz] = 0\n",
    "            ids.append(id_str)\n",
    "            try:\n",
    "                classes.append(util.malware_classes.index(clazz))\n",
    "            except ValueError:\n",
    "                assert clazz == \"X\"\n",
    "                classes.append(-1)\n",
    "            tree = ET.parse(os.path.join(direc,datafile))\n",
    "            fds.append(ff(tree, globalDict,secondTime))\n",
    "            del tree\n",
    "            \n",
    "    if testDict == None:\n",
    "        return np.array(fds), np.array(classes), ids, globalDict\n",
    "    return np.array(fds), np.array(classes), ids\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "\t\"\"\"\n",
    "\targuments:\n",
    "\t  fds is a list of feature dicts (one for each row).\n",
    "\t  global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "\t  should only be provided when extracting features from test data, so that \n",
    "\t  the columns of the test matrix align correctly.\n",
    "\t   \n",
    "\treturns: \n",
    "\t\ta sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "\t\tthe union of features defined in any of the fds \n",
    "\t\"\"\"\n",
    "\tif global_feat_dict is None:\n",
    "\t\tall_feats = set()\n",
    "\t\t[all_feats.update(fd.keys()) for fd in fds]\n",
    "\t\tfeat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "\telse:\n",
    "\t\tfeat_dict = global_feat_dict\n",
    "\t\t\n",
    "\tcols = []\n",
    "\trows = []\n",
    "\tdata = []\t\t\n",
    "\tfor i in range(len(fds)):\n",
    "\t\ttemp_cols = []\n",
    "\t\ttemp_data = []\n",
    "\t\tfor feat,val in fds[i].items():\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# update temp_cols iff update temp_data\n",
    "\t\t\t\ttemp_cols.append(feat_dict[feat])\n",
    "\t\t\t\ttemp_data.append(val)\n",
    "\t\t\texcept KeyError as ex:\n",
    "\t\t\t\tif global_feat_dict is not None:\n",
    "\t\t\t\t\tpass  # new feature in test data; nbd\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\traise ex\n",
    "\n",
    "\t\t# all fd's features in the same row\n",
    "\t\tk = len(temp_cols)\n",
    "\t\tcols.extend(temp_cols)\n",
    "\t\tdata.extend(temp_data)\n",
    "\t\trows.extend([i]*k)\n",
    "\n",
    "\tassert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "\n",
    "\tX = sparse.csr_matrix((np.array(data),\n",
    "\t\t\t\t   (np.array(rows), np.array(cols))),\n",
    "\t\t\t\t   shape=(len(fds), len(feat_dict)))\n",
    "\treturn X, feat_dict\n",
    "\t\n",
    "\n",
    "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# feature-names to numeric values.\n",
    "## TODO: modify these functions, and/or add new ones.\n",
    "def first_last_system_call_feats(tree):\n",
    "\t\"\"\"\n",
    "\targuments:\n",
    "\t  tree is an xml.etree.ElementTree object\n",
    "\treturns:\n",
    "\t  a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "\t  made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "\t  (in other words, it returns a dictionary indicating what the first and \n",
    "\t  last system calls made by an executable were.)\n",
    "\t\"\"\"\n",
    "\tc = Counter()\n",
    "\tin_all_section = False\n",
    "\tfirst = True # is this the first system call\n",
    "\tlast_call = None # keep track of last call we've seen\n",
    "\tfor el in tree.iter():\n",
    "\t\t# ignore everything outside the \"all_section\" element\n",
    "\t\tif el.tag == \"all_section\" and not in_all_section:\n",
    "\t\t\tin_all_section = True\n",
    "\t\telif el.tag == \"all_section\" and in_all_section:\n",
    "\t\t\tin_all_section = False\n",
    "\t\telif in_all_section:\n",
    "\t\t\tif first:\n",
    "\t\t\t\tc[\"first_call-\"+el.tag] = 1\n",
    "\t\t\t\tfirst = False\n",
    "\t\t\tlast_call = el.tag  # update last call seen\n",
    "\t\t\t\n",
    "\t# finally, mark last call seen\n",
    "\tc[\"last_call-\"+last_call] = 1\n",
    "\treturn c\n",
    "\n",
    "def system_call_count_feats(tree):\n",
    "\t\"\"\"\n",
    "\targuments:\n",
    "\t  tree is an xml.etree.ElementTree object\n",
    "\treturns:\n",
    "\t  a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "\t  made by an executable (summed over all processes)\n",
    "\t\"\"\"\n",
    "\tc = Counter()\n",
    "\tin_all_section = False\n",
    "\tfor el in tree.iter():\n",
    "\t\t# ignore everything outside the \"all_section\" element\n",
    "\t\tif el.tag == \"all_section\" and not in_all_section:\n",
    "\t\t\tin_all_section = True\n",
    "\t\telif el.tag == \"all_section\" and in_all_section:\n",
    "\t\t\tin_all_section = False\n",
    "\t\telif in_all_section:\n",
    "\t\t\tc['num_system_calls'] += 1\n",
    "\treturn c\n",
    "\n",
    "def full_Dictionary(tree):\n",
    "\tc = Counter()\n",
    "\tn=5\n",
    "\tin_all_section = False\n",
    "\tprev = []\n",
    "\tfor el in tree.iter():\n",
    "\t\t# ignore everything outside the \"all_section\" element\n",
    "\t\tif el.tag == \"all_section\" and not in_all_section:\n",
    "\t\t\tin_all_section = True\n",
    "\t\telif el.tag == \"all_section\" and in_all_section:\n",
    "\t\t\tin_all_section = False\n",
    "\t\telif in_all_section:\n",
    "\t\t\t'''stringIfied = str(el)\n",
    "\t\t\twordList = stringIfied.split()\n",
    "\t\t\tfor i in range(len(wordList)):\n",
    "\t\t\t\tif prev != [] and i < n:\n",
    "\t\t\t\t\tc[str(prev[len(prev)-n+i : ]+wordList[0:i])] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tc[str(wordList[i-n : i])] += 1\n",
    "\t\t\tprev = wordList'''\n",
    "\t\t\t#for (a,b) in el.attrib.items(): c[b] += 1\n",
    "            \n",
    "\treturn c\n",
    "\n",
    "def fullText(tree,wordDict,secondTime=False):\n",
    "    c = []\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            1 == 1\n",
    "            words = list(str(el))\n",
    "            for word in words:\n",
    "                if word in wordDict:\n",
    "                    c.append(wordDict[word])\n",
    "                elif wordDict[\"counter\"] < 20000 and secondTime == False:\n",
    "                    wordDict[word] = wordDict[\"counter\"]\n",
    "                    wordDict[\"counter\"] += 1\n",
    "                    c.append(wordDict[word])\n",
    "            del words\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training features...\n",
      "done extracting training features\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "outputfile = \"sample_predictions.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "# TODO put the names of the feature functions you've defined above in this list\n",
    "ffs = [first_last_system_call_feats, system_call_count_feats,full_Dictionary]\n",
    "\n",
    "# extract features\n",
    "print(\"extracting training features...\")\n",
    "X_train, t_train, train_ids,dictionary = extractText(fullText)\n",
    "#X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "print (\"done extracting training features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[\"counter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features...\n",
      "done extracting test features\n"
     ]
    }
   ],
   "source": [
    "print(\"extracting test features...\")\n",
    "X_test,t_test,test_ids = extractText(fullText,direc=test_dir,testDict=dictionary)\n",
    "print(\"done extracting test features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065468\n",
      "89989.25394548062\n"
     ]
    }
   ],
   "source": [
    "maxi = 0\n",
    "maxiAvg = 0\n",
    "for i in range(len(X_train)):\n",
    "    maxiAvg += len(X_train[i])\n",
    "    if (len(X_train[i])) > maxi:\n",
    "        maxi = len(X_train[i])\n",
    "print(maxi)\n",
    "print(maxiAvg/(len(X_train)))\n",
    "maxi = 3000\n",
    "#X_test1 = sequence.pad_sequences(X_test, maxlen=maxi)\n",
    "X_train1 = sequence.pad_sequences(X_train, maxlen=maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(41, embedding_vecor_length, input_length=maxi))\n",
    "#model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(128,dropout = .2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(41, activation = 'softmax'))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_10 to have shape (41,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-b222ca5f453b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fuck\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1638\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1488\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1489\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_10 to have shape (41,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train1,t_train,epochs = 3,batch_size=24)\n",
    "print(\"fuck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)\n",
    "for i in range(len(preds)):\n",
    "    print(np.argmax(preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X_trainNormalized=sklearn.preprocessing.normalize(X_train)\n",
    "model = Sequential()\n",
    "#model.add(Dense(units=64, activation='relu', input_dim=1219080))\n",
    "model.add(Embedding(1000, 64, input_length=))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(units=15, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "#learned_W = logReg.fit(X_train, np.array(t_train))\n",
    "y_train = keras.utils.to_categorical(np.array(t_train), num_classes=15)\n",
    "print(y_train)\n",
    "model.fit(X_trainNormalized, y_train, epochs=2, batch_size=32)\n",
    "#print(learned_W.get_params())\n",
    "print(\"done learning\")\n",
    "print()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extracting test features...\")\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print(\"done extracting test features\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"making predictions...\")\n",
    "#preds = np.argmax(X_test.dot(learned_W),axis=1)\n",
    "#preds = logReg.predict(X_test)\n",
    "preds = model.predict(sklearn.preprocessing.normalize(X_test), batch_size=128)\n",
    "preds1 = []\n",
    "print(len(preds))\n",
    "print(len(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_W = logReg.fit(X_train, np.array(t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)\n",
    "print(\"writing predictions...\")\n",
    "util.write_predictions(preds, test_ids, outputfile)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=keras.layers.SimpleRNN(15, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
